version: "3.1"

services:

  webserver:
    networks:
      - ndsnet
    env_file:
      - .env
    # volumes:
    #   - ./dags:/usr/local/airflow/dags

  scheduler:
    networks:
      - ndsnet
    env_file:
      - .env
    # volumes:
    #   - ./dags:/usr/local/airflow/dags

  triggerer:
    networks:
      - ndsnet
    env_file:
      - .env

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8082:8080"   # Spark UI (pode acessar via http://localhost:8082)
      - "7077:7077"   # Spark cluster port
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      - ndsnet

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"   # <== Libera a UI do executor para logs
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - ndsnet

  metabase:
    image: metabase/metabase:v0.50.4
    restart: always
    ports:
      - "3000:3000"
    volumes:
      - ./include/data/metabase:/metabase-data
    networks:
      - ndsnet

  docker-proxy:
    image: alpine/socat
    command: "TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock"
    ports:
      - "2376:2375"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - ndsnet

networks:
  ndsnet:
    driver: bridge
